{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#朴素贝叶斯-Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print myVocabList\n",
    "print listOPosts[0]\n",
    "print setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)      #change to ones() \n",
    "    p0Denom = 0.0; p1Denom = 0.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom          #change to log()\n",
    "    p0Vect = p0Num/p0Denom          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n",
      "[ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "\n",
    "trainMat = [setOfWords2Vec(myVocabList, postinDoc) for postinDoc in listOPosts ]\n",
    "\n",
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)\n",
    "\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "\n",
    "trainMat = [setOfWords2Vec(myVocabList, postinDoc) for postinDoc in listOPosts ]\n",
    "\n",
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)\n",
    "\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L I have ever laid eyes upon.'\n",
    "print mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx=re.compile('\\\\W*')\n",
    "listOTokens=regEx.split(mySent)\n",
    "print listOTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print [tok.lower() for tok in listOTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()\n",
    "listOTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print \"classification error\",docList[docIndex]\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]       \n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "638\n",
      "638\n",
      "the error rate is:  0.4\n"
     ]
    }
   ],
   "source": [
    "print len(vocabList)\n",
    "print len(pSF)\n",
    "print len(pNY)\n",
    "vocabList, pSF,pNY=localWords(ny,sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.5\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "this\n",
      "good\n",
      "live\n",
      "movies\n",
      "share\n",
      "talk\n",
      "then\n",
      "city\n",
      "too\n",
      "walk\n",
      "interested\n",
      "activities\n",
      "cool\n",
      "let\n",
      "use\n",
      "etc\n",
      "meet\n",
      "want\n",
      "how\n",
      "plans\n",
      "may\n",
      "maybe\n",
      "seeking\n",
      "got\n",
      "could\n",
      "sounds\n",
      "tonight\n",
      "retired\n",
      "watch\n",
      "please\n",
      "life\n",
      "fun\n",
      "older\n",
      "spend\n",
      "easy\n",
      "around\n",
      "know\n",
      "some\n",
      "back\n",
      "dinner\n",
      "friends\n",
      "area\n",
      "nice\n",
      "younger\n",
      "age\n",
      "hello\n",
      "conversation\n",
      "young\n",
      "send\n",
      "woman\n",
      "very\n",
      "void\n",
      "minute\n",
      "tri\n",
      "snuggled\n",
      "sun\n",
      "lights\n",
      "goes\n",
      "seeks\n",
      "men\n",
      "explore\n",
      "fantasies\n",
      "pics\n",
      "festivals\n",
      "select\n",
      "few\n",
      "means\n",
      "more\n",
      "relax\n",
      "company\n",
      "join\n",
      "room\n",
      "divorced\n",
      "work\n",
      "worth\n",
      "something\n",
      "clothing\n",
      "comedy\n",
      "pizza\n",
      "lay\n",
      "guys\n",
      "still\n",
      "thank\n",
      "interesting\n",
      "resort\n",
      "coffee\n",
      "safe\n",
      "now\n",
      "friendship\n",
      "concerts\n",
      "weight\n",
      "idea\n",
      "girl\n",
      "saturday\n",
      "space\n",
      "robert\n",
      "internet\n",
      "caucasian\n",
      "shows\n",
      "ramon\n",
      "58yr\n",
      "care\n",
      "transition\n",
      "days\n",
      "chilly\n",
      "dine\n",
      "dont\n",
      "long\n",
      "little\n",
      "weekdays\n",
      "free\n",
      "eyes\n",
      "way\n",
      "believe\n",
      "valley\n",
      "thx\n",
      "light\n",
      "interests\n",
      "san\n",
      "mind\n",
      "person\n",
      "offer\n",
      "also\n",
      "marin\n",
      "don\n",
      "away\n",
      "text\n",
      "normalaties\n",
      "pleasanton\n",
      "find\n",
      "going\n",
      "pretty\n",
      "new\n",
      "proportionate\n",
      "distraction\n",
      "naked\n",
      "common\n",
      "see\n",
      "movie\n",
      "shaved\n",
      "horny\n",
      "reading\n",
      "optional\n",
      "last\n",
      "taking\n",
      "april\n",
      "twin\n",
      "likes\n",
      "lifes\n",
      "companionship\n",
      "vacation\n",
      "been\n",
      "treat\n",
      "napa\n",
      "meeting\n",
      "wants\n",
      "photos\n",
      "has\n",
      "swim\n",
      "canceled\n",
      "layed\n",
      "partner\n",
      "open\n",
      "peaks\n",
      "hang\n",
      "running\n",
      "moving\n",
      "fucked\n",
      "hispanic\n",
      "preference\n",
      "half\n",
      "thanks\n",
      "cut\n",
      "dslr\n",
      "dining\n",
      "lets\n",
      "kick\n",
      "take\n",
      "early\n",
      "possibly\n",
      "discreet\n",
      "height\n",
      "lady\n",
      "semi\n",
      "either\n",
      "night\n",
      "become\n",
      "tower\n",
      "old\n",
      "often\n",
      "30s\n",
      "clubs\n",
      "palm\n",
      "springs\n",
      "daytime\n",
      "decent\n",
      "wanna\n",
      "heater\n",
      "your\n",
      "lot\n",
      "yrs\n",
      "mundane\n",
      "hear\n",
      "mature\n",
      "physical\n",
      "ideally\n",
      "evenings\n",
      "problems\n",
      "fresh\n",
      "weekend\n",
      "pool\n",
      "required\n",
      "far\n",
      "intimacy\n",
      "all\n",
      "month\n",
      "sleep\n",
      "hate\n",
      "yummy\n",
      "buddy\n",
      "tush\n",
      "helps\n",
      "masculine\n",
      "must\n",
      "friendly\n",
      "mwm\n",
      "minded\n",
      "xxxxxxxxxx\n",
      "betterment\n",
      "respectful\n",
      "kneading\n",
      "large\n",
      "cmt\n",
      "enjoy\n",
      "dealing\n",
      "video\n",
      "drinks\n",
      "what\n",
      "fucks\n",
      "current\n",
      "indian\n",
      "reply\n",
      "told\n",
      "body\n",
      "outgoing\n",
      "empower\n",
      "introverted\n",
      "healing\n",
      "relaxation\n",
      "strong\n",
      "change\n",
      "great\n",
      "trans\n",
      "gent\n",
      "partakes\n",
      "allows\n",
      "experience\n",
      "love\n",
      "prefer\n",
      "total\n",
      "fee\n",
      "two\n",
      "sports\n",
      "music\n",
      "wondering\n",
      "type\n",
      "tell\n",
      "females\n",
      "knows\n",
      "club\n",
      "astoria\n",
      "warm\n",
      "basically\n",
      "translatina4men\n",
      "hola\n",
      "women\n",
      "include\n",
      "entertained\n",
      "cuddle\n",
      "nine\n",
      "entertainer\n",
      "closing\n",
      "male\n",
      "give\n",
      "staten\n",
      "accept\n",
      "high\n",
      "heard\n",
      "his\n",
      "times\n",
      "native\n",
      "swedish\n",
      "needs\n",
      "thing\n",
      "travel\n",
      "watching\n",
      "amistad\n",
      "busco\n",
      "amazing\n",
      "toxic\n",
      "nyc\n",
      "mas\n",
      "fran\n",
      "slush\n",
      "9589escape\n",
      "ethnic\n",
      "suck\n",
      "realized\n",
      "elusive\n",
      "appreciate\n",
      "honestly\n",
      "enpesar\n",
      "tall\n",
      "playing\n",
      "cute\n",
      "office\n",
      "over\n",
      "move\n",
      "years\n",
      "through\n",
      "shake\n",
      "its\n",
      "pent\n",
      "group\n",
      "neighborly\n",
      "personal\n",
      "denver\n",
      "better\n",
      "willing\n",
      "cutie\n",
      "food\n",
      "dan\n",
      "dresscode\n",
      "therapy\n",
      "they\n",
      "hands\n",
      "term\n",
      "always\n",
      "each\n",
      "addicted\n",
      "found\n",
      "went\n",
      "glasses\n",
      "energy\n",
      "year\n",
      "our\n",
      "try\n",
      "furthe\n",
      "tensions\n",
      "since\n",
      "trauma\n",
      "attendee\n",
      "turning\n",
      "friday\n",
      "envelop\n",
      "quite\n",
      "complicated\n",
      "wanted\n",
      "craving\n",
      "benefits\n",
      "keep\n",
      "blood\n",
      "streight\n",
      "nothi\n",
      "feel\n",
      "yoni\n",
      "one\n",
      "another\n",
      "drinker\n",
      "miss\n",
      "necessarily\n",
      "quality\n",
      "needed\n",
      "divorce\n",
      "their\n",
      "white\n",
      "las\n",
      "makeout\n",
      "discussion\n",
      "relationship\n",
      "foreplay\n",
      "serve\n",
      "stress\n",
      "seabright\n",
      "natural\n",
      "kind\n",
      "double\n",
      "kink\n",
      "bed\n",
      "matter\n",
      "feeling\n",
      "comfortable\n",
      "say\n",
      "beefy\n",
      "seek\n",
      "any\n",
      "nights\n",
      "para\n",
      "self\n",
      "snow\n",
      "lit\n",
      "latina\n",
      "which\n",
      "cleft\n",
      "skinned\n",
      "pain\n",
      "normal\n",
      "most\n",
      "brooklyn\n",
      "regular\n",
      "queens\n",
      "muscle\n",
      "drive\n",
      "traditional\n",
      "tech\n",
      "clean\n",
      "professional\n",
      "skinny\n",
      "sector\n",
      "show\n",
      "meetings\n",
      "queen\n",
      "brief\n",
      "therapist\n",
      "slow\n",
      "nuturing\n",
      "acne\n",
      "behind\n",
      "spirituality\n",
      "only\n",
      "black\n",
      "touched\n",
      "trade\n",
      "uncontested\n",
      "hit\n",
      "horrible\n",
      "bear\n",
      "nasty\n",
      "shy\n",
      "areas\n",
      "married\n",
      "twice\n",
      "where\n",
      "hairy\n",
      "bars\n",
      "bushwich\n",
      "fascinating\n",
      "sex\n",
      "close\n",
      "best\n",
      "347base\n",
      "wow\n",
      "currently\n",
      "enough\n",
      "between\n",
      "neither\n",
      "berkeley\n",
      "warrior\n",
      "never\n",
      "recently\n",
      "missing\n",
      "tantra\n",
      "attention\n",
      "job\n",
      "both\n",
      "commands\n",
      "many\n",
      "anitime\n",
      "minds\n",
      "connection\n",
      "games\n",
      "inspired\n",
      "comes\n",
      "headed\n",
      "simple\n",
      "others\n",
      "whatever\n",
      "table\n",
      "heals\n",
      "come\n",
      "fro\n",
      "secret\n",
      "much\n",
      "educated\n",
      "gay\n",
      "concerning\n",
      "moment\n",
      "lives\n",
      "deep\n",
      "smoking\n",
      "cumpany\n",
      "those\n",
      "chill\n",
      "multi\n",
      "professionally\n",
      "will\n",
      "while\n",
      "suppose\n",
      "hav\n",
      "guide\n",
      "seven\n",
      "almost\n",
      "prayer\n",
      "despues\n",
      "tissue\n",
      "eclecticism\n",
      "ready\n",
      "rid\n",
      "everyday\n",
      "chauffeur\n",
      "cuisine\n",
      "xxxxxxxxxxxxxxx\n",
      "generic\n",
      "things\n",
      "make\n",
      "tour\n",
      "same\n",
      "check\n",
      "complex\n",
      "events\n",
      "week\n",
      "meets\n",
      "slave\n",
      "drink\n",
      "weed\n",
      "kik\n",
      "humble\n",
      "well\n",
      "smoker\n",
      "obviously\n",
      "hearty\n",
      "relief\n",
      "chemistry\n",
      "knowledgeable\n",
      "proximity\n",
      "clot\n",
      "touch\n",
      "playtime\n",
      "yes\n",
      "yet\n",
      "had\n",
      "color\n",
      "book\n",
      "transformed\n",
      "east\n",
      "real\n",
      "submissive\n",
      "mujer\n",
      "read\n",
      "supervise\n",
      "world\n",
      "desire\n",
      "una\n",
      "cosas\n",
      "passionate\n",
      "soft\n",
      "beneficial\n",
      "buddies\n",
      "people\n",
      "pushy\n",
      "sure\n",
      "curious\n",
      "txt\n",
      "pet\n",
      "pues\n",
      "worship\n",
      "forgets\n",
      "obey\n",
      "rub\n",
      "pressure\n",
      "host\n",
      "post\n",
      "intense\n",
      "working\n",
      "filipino\n",
      "netflix\n",
      "island\n",
      "side\n",
      "drama\n",
      "algo\n",
      "subs\n",
      "wasting\n",
      "into\n",
      "within\n",
      "down\n",
      "weather\n",
      "female\n",
      "methods\n",
      "her\n",
      "spam\n",
      "tingles\n",
      "hey\n",
      "lol\n",
      "start\n",
      "inner\n",
      "quirky\n",
      "was\n",
      "form\n",
      "867maybe\n",
      "constantly\n",
      "relaxed\n",
      "feminine\n",
      "introvert\n",
      "glutes\n",
      "trying\n",
      "mensage\n",
      "bud\n",
      "handsome\n",
      "throw\n",
      "dangerous\n",
      "attached\n",
      "exploration\n",
      "nerdy\n",
      "emotional\n",
      "similar\n",
      "strongly\n",
      "cinema\n",
      "pic\n",
      "moved\n",
      "doesn\n",
      "single\n",
      "cure\n",
      "right\n",
      "candle\n",
      "annoying\n",
      "trip\n",
      "compared\n",
      "when\n",
      "really\n",
      "greenery\n",
      "monday\n",
      "anal\n",
      "lead\n",
      "home\n",
      "understand\n",
      "oral\n",
      "having\n",
      "once\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "those\n",
      "tush\n",
      "more\n",
      "another\n",
      "forgets\n",
      "glutes\n",
      "enjoy\n",
      "something\n",
      "energy\n",
      "yoni\n",
      "going\n",
      "things\n",
      "make\n",
      "hang\n",
      "take\n",
      "woman\n",
      "very\n",
      "kneading\n",
      "body\n",
      "men\n",
      "let\n",
      "healing\n",
      "sports\n",
      "tell\n",
      "women\n",
      "want\n",
      "amazing\n",
      "guys\n",
      "playing\n",
      "years\n",
      "seeking\n",
      "therapy\n",
      "they\n",
      "always\n",
      "try\n",
      "streight\n",
      "feel\n",
      "their\n",
      "too\n",
      "white\n",
      "relationship\n",
      "stress\n",
      "natural\n",
      "say\n",
      "also\n",
      "new\n",
      "areas\n",
      "close\n",
      "wow\n",
      "attention\n",
      "both\n",
      "been\n",
      "wants\n",
      "life\n",
      "educated\n",
      "lives\n",
      "will\n",
      "same\n",
      "relief\n",
      "touch\n",
      "old\n",
      "often\n",
      "rub\n",
      "wanna\n",
      "into\n",
      "female\n",
      "your\n",
      "methods\n",
      "friends\n",
      "hear\n",
      "sleep\n",
      "hate\n",
      "yummy\n",
      "buddy\n",
      "helps\n",
      "masculine\n",
      "mwm\n",
      "activities\n",
      "betterment\n",
      "respectful\n",
      "dealing\n",
      "video\n",
      "what\n",
      "current\n",
      "empower\n",
      "relaxation\n",
      "strong\n",
      "change\n",
      "great\n",
      "trans\n",
      "partakes\n",
      "allows\n",
      "experience\n",
      "pics\n",
      "prefer\n",
      "fee\n",
      "live\n",
      "type\n",
      "company\n",
      "astoria\n",
      "basically\n",
      "translatina4men\n",
      "this\n",
      "work\n",
      "cuddle\n",
      "entertainer\n",
      "meet\n",
      "male\n",
      "give\n",
      "high\n",
      "heard\n",
      "native\n",
      "thing\n",
      "watching\n",
      "toxic\n",
      "nyc\n",
      "may\n",
      "9589escape\n",
      "honestly\n",
      "talk\n",
      "office\n",
      "through\n",
      "shake\n",
      "its\n",
      "interesting\n",
      "better\n",
      "coffee\n",
      "food\n",
      "hands\n",
      "now\n",
      "friendship\n",
      "year\n",
      "our\n",
      "since\n",
      "trauma\n",
      "turning\n",
      "complicated\n",
      "wanted\n",
      "craving\n",
      "benefits\n",
      "keep\n",
      "blood\n",
      "long\n",
      "little\n",
      "divorce\n",
      "free\n",
      "believe\n",
      "bed\n",
      "matter\n",
      "feeling\n",
      "self\n",
      "which\n",
      "pain\n",
      "normal\n",
      "regular\n",
      "queens\n",
      "muscle\n",
      "traditional\n",
      "clean\n",
      "therapist\n",
      "find\n",
      "slow\n",
      "trade\n",
      "uncontested\n",
      "nasty\n",
      "shy\n",
      "married\n",
      "where\n",
      "bushwich\n",
      "fascinating\n",
      "see\n",
      "347base\n",
      "currently\n",
      "enough\n",
      "between\n",
      "warrior\n",
      "tantra\n",
      "job\n",
      "many\n",
      "taking\n",
      "connection\n",
      "games\n",
      "comes\n",
      "simple\n",
      "others\n",
      "heals\n",
      "secret\n",
      "much\n",
      "gay\n",
      "concerning\n",
      "moment\n",
      "deep\n",
      "smoking\n",
      "chill\n",
      "while\n",
      "suppose\n",
      "hav\n",
      "almost\n",
      "eclecticism\n",
      "rid\n",
      "everyday\n",
      "cuisine\n",
      "generic\n",
      "events\n",
      "week\n",
      "kik\n",
      "well\n",
      "smoker\n",
      "chemistry\n",
      "proximity\n",
      "yet\n",
      "had\n",
      "easy\n",
      "real\n",
      "read\n",
      "discreet\n",
      "world\n",
      "lady\n",
      "buddies\n",
      "people\n",
      "curious\n",
      "pressure\n",
      "host\n",
      "post\n",
      "working\n",
      "side\n",
      "drama\n",
      "wasting\n",
      "within\n",
      "down\n",
      "her\n",
      "inner\n",
      "quirky\n",
      "was\n",
      "form\n",
      "867maybe\n",
      "constantly\n",
      "feminine\n",
      "introvert\n",
      "trying\n",
      "bud\n",
      "dangerous\n",
      "attached\n",
      "emotional\n",
      "strongly\n",
      "cinema\n",
      "single\n",
      "cure\n",
      "physical\n",
      "greenery\n",
      "weekend\n",
      "younger\n",
      "home\n",
      "hello\n",
      "having\n",
      "once\n",
      "all\n",
      "month\n",
      "conversation\n",
      "young\n",
      "send\n",
      "must\n",
      "friendly\n",
      "void\n",
      "minded\n",
      "xxxxxxxxxx\n",
      "minute\n",
      "cool\n",
      "tri\n",
      "large\n",
      "cmt\n",
      "snuggled\n",
      "drinks\n",
      "sun\n",
      "fucks\n",
      "lights\n",
      "indian\n",
      "goes\n",
      "reply\n",
      "seeks\n",
      "told\n",
      "outgoing\n",
      "explore\n",
      "introverted\n",
      "fantasies\n",
      "gent\n",
      "love\n",
      "festivals\n",
      "total\n",
      "select\n",
      "use\n",
      "etc\n",
      "two\n",
      "few\n",
      "music\n",
      "wondering\n",
      "means\n",
      "females\n",
      "knows\n",
      "relax\n",
      "club\n",
      "warm\n",
      "hola\n",
      "include\n",
      "join\n",
      "room\n",
      "divorced\n",
      "entertained\n",
      "worth\n",
      "movies\n",
      "nine\n",
      "closing\n",
      "share\n",
      "staten\n",
      "accept\n",
      "his\n",
      "times\n",
      "clothing\n",
      "swedish\n",
      "needs\n",
      "travel\n",
      "amistad\n",
      "busco\n",
      "how\n",
      "comedy\n",
      "pizza\n",
      "mas\n",
      "plans\n",
      "fran\n",
      "slush\n",
      "lay\n",
      "ethnic\n",
      "suck\n",
      "realized\n",
      "elusive\n",
      "maybe\n",
      "appreciate\n",
      "enpesar\n",
      "tall\n",
      "cute\n",
      "over\n",
      "move\n",
      "still\n",
      "pent\n",
      "group\n",
      "thank\n",
      "neighborly\n",
      "personal\n",
      "denver\n",
      "resort\n",
      "willing\n",
      "cutie\n",
      "then\n",
      "dan\n",
      "safe\n",
      "dresscode\n",
      "term\n",
      "good\n",
      "each\n",
      "addicted\n",
      "found\n",
      "went\n",
      "concerts\n",
      "glasses\n",
      "weight\n",
      "idea\n",
      "girl\n",
      "saturday\n",
      "furthe\n",
      "space\n",
      "tensions\n",
      "robert\n",
      "internet\n",
      "got\n",
      "caucasian\n",
      "shows\n",
      "attendee\n",
      "friday\n",
      "envelop\n",
      "ramon\n",
      "quite\n",
      "58yr\n",
      "care\n",
      "could\n",
      "transition\n",
      "days\n",
      "chilly\n",
      "dine\n",
      "dont\n",
      "nothi\n",
      "one\n",
      "drinker\n",
      "sounds\n",
      "miss\n",
      "city\n",
      "necessarily\n",
      "quality\n",
      "needed\n",
      "weekdays\n",
      "las\n",
      "makeout\n",
      "discussion\n",
      "eyes\n",
      "way\n",
      "foreplay\n",
      "serve\n",
      "seabright\n",
      "kind\n",
      "valley\n",
      "double\n",
      "thx\n",
      "kink\n",
      "light\n",
      "interests\n",
      "san\n",
      "mind\n",
      "comfortable\n",
      "tonight\n",
      "beefy\n",
      "seek\n",
      "any\n",
      "nights\n",
      "para\n",
      "person\n",
      "offer\n",
      "snow\n",
      "lit\n",
      "latina\n",
      "cleft\n",
      "skinned\n",
      "retired\n",
      "marin\n",
      "most\n",
      "brooklyn\n",
      "don\n",
      "drive\n",
      "tech\n",
      "professional\n",
      "skinny\n",
      "away\n",
      "sector\n",
      "show\n",
      "text\n",
      "meetings\n",
      "queen\n",
      "brief\n",
      "normalaties\n",
      "pleasanton\n",
      "nuturing\n",
      "acne\n",
      "behind\n",
      "spirituality\n",
      "only\n",
      "black\n",
      "touched\n",
      "pretty\n",
      "hit\n",
      "horrible\n",
      "watch\n",
      "bear\n",
      "proportionate\n",
      "distraction\n",
      "naked\n",
      "twice\n",
      "common\n",
      "hairy\n",
      "bars\n",
      "sex\n",
      "best\n",
      "movie\n",
      "please\n",
      "shaved\n",
      "horny\n",
      "neither\n",
      "reading\n",
      "berkeley\n",
      "never\n",
      "recently\n",
      "missing\n",
      "optional\n",
      "commands\n",
      "last\n",
      "anitime\n",
      "minds\n",
      "april\n",
      "inspired\n",
      "headed\n",
      "whatever\n",
      "walk\n",
      "twin\n",
      "likes\n",
      "table\n",
      "lifes\n",
      "companionship\n",
      "come\n",
      "fro\n",
      "vacation\n",
      "treat\n",
      "napa\n",
      "meeting\n",
      "photos\n",
      "cumpany\n",
      "multi\n",
      "has\n",
      "professionally\n",
      "fun\n",
      "guide\n",
      "swim\n",
      "seven\n",
      "prayer\n",
      "canceled\n",
      "despues\n",
      "tissue\n",
      "layed\n",
      "ready\n",
      "partner\n",
      "chauffeur\n",
      "open\n",
      "xxxxxxxxxxxxxxx\n",
      "tour\n",
      "check\n",
      "complex\n",
      "meets\n",
      "slave\n",
      "peaks\n",
      "drink\n",
      "weed\n",
      "running\n",
      "moving\n",
      "fucked\n",
      "older\n",
      "humble\n",
      "obviously\n",
      "hispanic\n",
      "hearty\n",
      "spend\n",
      "preference\n",
      "knowledgeable\n",
      "half\n",
      "clot\n",
      "thanks\n",
      "playtime\n",
      "yes\n",
      "cut\n",
      "dslr\n",
      "dining\n",
      "color\n",
      "lets\n",
      "book\n",
      "transformed\n",
      "east\n",
      "kick\n",
      "submissive\n",
      "mujer\n",
      "around\n",
      "early\n",
      "possibly\n",
      "know\n",
      "supervise\n",
      "height\n",
      "desire\n",
      "semi\n",
      "una\n",
      "cosas\n",
      "either\n",
      "night\n",
      "become\n",
      "tower\n",
      "passionate\n",
      "soft\n",
      "beneficial\n",
      "pushy\n",
      "30s\n",
      "some\n",
      "back\n",
      "clubs\n",
      "sure\n",
      "palm\n",
      "springs\n",
      "txt\n",
      "pet\n",
      "pues\n",
      "daytime\n",
      "worship\n",
      "obey\n",
      "intense\n",
      "filipino\n",
      "netflix\n",
      "island\n",
      "dinner\n",
      "algo\n",
      "decent\n",
      "heater\n",
      "subs\n",
      "weather\n",
      "spam\n",
      "tingles\n",
      "hey\n",
      "lol\n",
      "start\n",
      "lot\n",
      "yrs\n",
      "mundane\n",
      "relaxed\n",
      "mensage\n",
      "handsome\n",
      "throw\n",
      "exploration\n",
      "mature\n",
      "nerdy\n",
      "similar\n",
      "pic\n",
      "moved\n",
      "doesn\n",
      "right\n",
      "candle\n",
      "annoying\n",
      "trip\n",
      "area\n",
      "compared\n",
      "ideally\n",
      "when\n",
      "interested\n",
      "evenings\n",
      "really\n",
      "nice\n",
      "monday\n",
      "problems\n",
      "anal\n",
      "fresh\n",
      "pool\n",
      "lead\n",
      "age\n",
      "required\n",
      "far\n",
      "understand\n",
      "intimacy\n",
      "oral\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
